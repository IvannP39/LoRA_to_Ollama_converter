# LoRA to Ollama Converter

Application GUI permettant de convertir facilement vos mod√®les LoRA fine-tuned en mod√®les compatibles Ollama.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Platform](https://img.shields.io/badge/Platform-Windows%20%7C%20Linux%20%7C%20macOS-lightgrey.svg)

## Table des mati√®res

- [Aper√ßu](#aper√ßu)
- [Fonctionnalit√©s](#fonctionnalit√©s)
- [Pr√©requis](#pr√©requis)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [Workflow de conversion](#workflow-de-conversion)
- [Configuration avanc√©e](#configuration-avanc√©e)
- [D√©pannage](#d√©pannage)
- [Exemple](#exemple)
- [Auteur](#auteur)

## Aper√ßu

**LoRA to Ollama Converter** est une application graphique intuitive qui automatise le processus de conversion d'un mod√®le LoRA (Low-Rank Adaptation) fine-tun√© vers un format compatible avec Ollama. Elle g√®re automatiquement :

- La mise √† jour de la configuration du mod√®le de base
- La conversion LoRA ‚Üí GGUF via llama.cpp
- La cr√©ation du Modelfile Ollama
- Le t√©l√©chargement automatique des mod√®les de base depuis HuggingFace
- L'enregistrement du mod√®le dans Ollama

![Image de l'interface](Images/gui.png)

## Fonctionnalit√©s

### Gestion des fichiers LoRA
- S√©lection intuitive des fichiers `adapter_model.safetensors` et `adapter_config.json`
- Chargement automatique du `base_model_name_or_path` depuis la configuration
- Modification facile du nom du mod√®le de base (ex: retirer `-bnb-4bit`)

### Sources de mod√®les de base flexibles
- **HuggingFace** : T√©l√©chargement automatique depuis n'importe quel repo
  - Support des mod√®les priv√©s/gated avec token HF
  - Gestion automatique du cache
- **Fichier local** : Utilisation d'un fichier GGUF d√©j√† t√©l√©charg√©

### Int√©gration llama.cpp
- T√©l√©chargement automatique de llama.cpp si non pr√©sent
- Ou utilisation d'une installation existante
- Conversion automatique LoRA ‚Üí GGUF

### Configuration Modelfile avanc√©e
- **Templates pr√©d√©finis** :
  - ChatML (Qwen, etc.)
  - Llama 3
  - Mistral/Mixtral
  - Alpaca
  - Vicuna
  - Custom (√©ditable)
- **Param√®tres personnalisables** :
  - Temperature
  - Top P
  - Top K
  - Num Context
- **System prompt** optionnel
- **Stop tokens** automatiques selon le template

### Sortie et logs
- Nom personnalisable du mod√®le Ollama final
- Dossier de sortie configurable
- Logs d√©taill√©s en temps r√©el avec codes couleur
- Barre de progression

## Pr√©requis

### Logiciels requis

- **Python 3.8+** : [T√©l√©charger Python](https://www.python.org/downloads/)
- **Git** : [T√©l√©charger Git](https://git-scm.com/downloads)
- **Ollama** : [Installer Ollama](https://ollama.ai/download)

### Biblioth√®ques Python

L'application utilise uniquement des biblioth√®ques standard Python :
- `tkinter` 
- `json`
- `subprocess`
- `os`
- `threading`
- `pathlib`

## Installation

### 1. Cloner ou t√©l√©charger le projet

```bash
git clone https://github.com/IvannP39/LoRA_to_Ollama_converter.git
cd LoRA_to_Ollama_converter
```

Ou t√©l√©chargez simplement le fichier `Lora_to_Ollama.py`.

### 2. Installer les d√©pendances (en vrai il n'y en a pas donc ne le faite pas :) )

```bash
pip install -r requirements.txt
```

### 3. V√©rifier Python

```bash
python --version
# ou
python3 --version
```

Assurez-vous d'avoir Python 3.8 ou sup√©rieur.

### 4. Installer Ollama

Suivez les instructions sur [ollama.ai](https://ollama.ai/download) pour votre syst√®me d'exploitation.

V√©rifiez l'installation :
```bash
ollama --version
```

## Utilisation

### Lancement de l'application

```bash
python Lora_to_Ollama.py
```

Ou sur certains syst√®mes :
```bash
python3 Lora_to_Ollama.py
```

### Guide pas √† pas

#### 1. Fichiers LoRA

1. Cliquez sur **üìÇ** √† c√¥t√© de "Chemin vers adapter_model.safetensors"
2. S√©lectionnez votre fichier `adapter_model.safetensors`
3. Cliquez sur **üìÇ** √† c√¥t√© de "Chemin vers adapter_config.json"
4. S√©lectionnez votre fichier `adapter_config.json`
5. Cliquez sur **Charger** pour r√©cup√©rer le `base_model_name_or_path`
6. **Important** : Modifiez le nom si n√©cessaire (ex: retirez `-bnb-4bit` ou `-GGUF`)

#### 2. Mod√®le de base

**Option A : HuggingFace**
1. S√©lectionnez **HuggingFace**
2. Entrez le nom du repo (ex: `unsloth/llama-3-8b`)
3. Si le mod√®le est priv√©/gated, entrez votre token HF

**Option B : Fichier local**
1. S√©lectionnez **Fichier local**
2. Cliquez sur **üìÇ** et s√©lectionnez votre fichier `.gguf`

#### 3. llama.cpp

- **Laisser vide** pour un t√©l√©chargement automatique
- **Ou** sp√©cifier le chemin vers une installation existante

#### 4. Configuration Modelfile

1. **Template** : Choisissez le template correspondant √† votre mod√®le
   - ChatML pour Qwen, Yi, etc.
   - Llama 3 pour les mod√®les Llama 3.x
   - Mistral/Mixtral pour ces familles
   - Custom pour un template personnalis√©

2. **System Prompt** (optionnel) : D√©finissez le comportement par d√©faut
   ```
   Tu es un assistant IA serviable et pr√©cis.
   ```

3. **Param√®tres** (optionnels, valeurs par d√©faut si vides) :
   - **Temperature** : 0.7 (cr√©ativit√©)
   - **Top P** : 0.9 (diversit√©)
   - **Top K** : 40 (limitation des tokens)
   - **Num Ctx** : 4096 (taille du contexte)

#### 5. Sortie

1. **Nom du mod√®le** : Choisissez un nom pour votre mod√®le Ollama
   ```
   mon-modele-custom
   ```

2. **Dossier de sortie** : S√©lectionnez o√π sauvegarder les fichiers g√©n√©r√©s

#### 6. Conversion

1. Cliquez sur **Convertir et Cr√©er le Mod√®le**
2. Suivez la progression dans les logs
3. Une fois termin√©, testez votre mod√®le :
   ```bash
   ollama run mon-modele-custom
   ```

## Workflow de conversion

L'application effectue automatiquement les √©tapes suivantes :

```mermaid
graph TD
    A[D√©but] --> B[Validation des entr√©es]
    B --> C[Mise √† jour adapter_config.json]
    C --> D{llama.cpp pr√©sent?}
    D -->|Non| E[Clone llama.cpp]
    D -->|Oui| F[Conversion LoRA ‚Üí GGUF]
    E --> F
    F --> G{Mod√®le de base?}
    G -->|HuggingFace| H[T√©l√©chargement via Ollama]
    G -->|Local| I[Utilisation fichier local]
    H --> J[G√©n√©ration Modelfile]
    I --> J
    J --> K[Cr√©ation mod√®le Ollama]
    K --> L[V√©rification mod√®le]
    L --> M[Fin ‚úÖ]
```

### D√©tails techniques

1. **Mise √† jour de la configuration** : Modifie `base_model_name_or_path` dans `adapter_config.json`

2. **Installation llama.cpp** (si n√©cessaire) :
   ```bash
   git clone https://github.com/ggerganov/llama.cpp.git
   ```

3. **Conversion LoRA ‚Üí GGUF** :
   ```bash
   python llama.cpp/convert_lora_to_gguf.py \
     --base base_model_name \
     adapter_model.safetensors \
     --outfile output.gguf
   ```

4. **G√©n√©ration du Modelfile** :
   ```dockerfile
   FROM base_model.gguf
   ADAPTER lora_adapter.gguf
   TEMPLATE """..."""
   SYSTEM """..."""
   PARAMETER temperature 0.7
   PARAMETER top_p 0.9
   PARAMETER top_k 40
   PARAMETER num_ctx 4096
   STOP "<|im_end|>"
   ```

5. **Cr√©ation du mod√®le** :
   ```bash
   ollama create mon-modele -f Modelfile
   ```

## Configuration avanc√©e

### Templates personnalis√©s

Pour cr√©er un template custom, s√©lectionnez "Custom" dans la liste d√©roulante et √©ditez le texte. Utilisez les variables Ollama :

- `{{ .System }}` : System prompt
- `{{ .Prompt }}` : Message utilisateur
- `{{ .Response }}` : R√©ponse du mod√®le

Exemple pour un nouveau format :
```
[SYSTEM]{{ .System }}[/SYSTEM]
[USER]{{ .Prompt }}[/USER]
[ASSISTANT]{{ .Response }}[/ASSISTANT]
```

### Variables d'environnement

Vous pouvez d√©finir ces variables pour personnaliser le comportement :

```bash
# Token HuggingFace (alternative √† la saisie dans l'interface)
export HF_TOKEN="hf_xxxxxxxxxx"

# Dossier de cache pour les mod√®les HuggingFace
export HF_HOME="/path/to/cache"
```

### Param√®tres de g√©n√©ration

| Param√®tre | Description | Valeur par d√©faut | Plage recommand√©e |
|-----------|-------------|-------------------|-------------------|
| **temperature** | Contr√¥le la cr√©ativit√© (plus √©lev√© = plus cr√©atif) | 0.7 | 0.1 - 2.0 |
| **top_p** | Nucleus sampling (diversit√© des tokens) | 0.9 | 0.1 - 1.0 |
| **top_k** | Limite le nombre de tokens consid√©r√©s | 40 | 1 - 100 |
| **num_ctx** | Taille de la fen√™tre de contexte | 4096 | 512 - 32768 |

## D√©pannage

### Probl√®me : "Ollama command not found"

**Solution** : Assurez-vous qu'Ollama est install√© et dans votre PATH.

```bash
# Windows (PowerShell)
$env:Path += ";C:\Program Files\Ollama"

# Linux/macOS
export PATH=$PATH:/usr/local/bin
```

### Probl√®me : "Failed to clone llama.cpp"

**Solution** : V√©rifiez votre connexion internet et que Git est install√©.

```bash
git --version
```

Si Git n'est pas install√©, t√©l√©chargez-le depuis [git-scm.com](https://git-scm.com/).

### Probl√®me : "Permission denied" lors de la conversion

**Solution** : Ex√©cutez l'application avec les permissions appropri√©es ou changez le dossier de sortie.

```bash
# Linux/macOS
chmod +x Lora_to_Ollama.py
```

### Probl√®me : "Model not found in Ollama"

**Solution** : Attendez quelques secondes que le mod√®le soit enregistr√©, puis v√©rifiez :

```bash
ollama list
```

Si le mod√®le n'appara√Æt pas, v√©rifiez les logs pour des erreurs de cr√©ation.

### Probl√®me : Erreur de m√©moire lors du t√©l√©chargement

**Solution** : Les gros mod√®les n√©cessitent beaucoup de RAM. Essayez :
- Fermer les autres applications
- Utiliser un mod√®le quantifi√© plus petit
- Augmenter la m√©moire swap

## Ressources utiles

- [Documentation Ollama](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)
- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)
- [Guide LoRA](https://huggingface.co/docs/peft/conceptual_guides/lora)
- [HuggingFace Hub](https://huggingface.co/models)

## Changelog

### Version 1.0.0 (2025-12-19)
- Version initiale
- Interface graphique moderne
- Support LoRA ‚Üí GGUF ‚Üí Ollama
- Support HuggingFace et fichiers locaux
- Templates pr√©d√©finis
- Logs d√©taill√©s en temps r√©el

## License

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

## Exemple

Voici un exemple de conversion et de test avec un fine-tuning QLoRA sur `Llama-3-8B-instruct-bnb-4bit` avec un tout petit dataset d'une cinquantaine de messages. L'objectif du mod√®le est de d√©finir le label du mail pass√© en prompt :

![Extrait du dataset](Images/extrait_dataset.png)
![Test du mod√®le](Images/ollama_test_model.png)
![Test du mod√®le](Images/ollama_test_model_2.png)

## Auteur

**Ivann**
- Ing√©nieur en √©lectronique pour l'embarqu√©
- 23 ans
- Date de cr√©ation : 19 d√©cembre 2025

## Remerciements

- [Ollama](https://ollama.ai/) pour leur excellent runtime LLM
- [llama.cpp](https://github.com/ggerganov/llama.cpp) pour les outils de conversion
- [HuggingFace](https://huggingface.co/) pour l'h√©bergement des mod√®les
- La communaut√© open-source pour le support et les retours

---

PS : Ce projet a √©t√© "vibe-cod√©" comme disent les jeuns.

<div align="center">

**Si ce projet vous a √©t√© utile, n'h√©sitez pas √† lui donner une √©toile !**

</div>
